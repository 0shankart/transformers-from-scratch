{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### **Self-Attention**"
      ],
      "metadata": {
        "id": "0AzUV-YA2GWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The whole idea of transformers and its related architecutures are based on the \"self-attention\" operation.<br><br><br>\n",
        "Self-attention is a sequence-to-sequence operation: a sequence of vector goes in, and a sequence of vector comes out. Let's input vectors $\\mathbf{x_1, x_2, x_3,...,x_t}$ and output vectors $\\mathbf{y_1, y_2, y_3,..., y_t}$ and all vectors have $\\mathbb{k}$ dimensions.<br>\n",
        "To produce output vector $\\mathbf{y_i}$, the self attention operation simply takes a weighted average over all the input vectors<br><br>\n",
        "$$\\mathbf{y_i} = \\sum_j \\mathbf{w}_{ij}\\mathbf{x}_j$$\n",
        "where $j$ indexes over the whole sequence and the weights sum to one over all $j$. The weight $\\mathbf{w_{ij}}$ is not a parameter, as in a normal neural net, but it is derived from a function over $\\mathbf{x_i}$ and $\\mathbf{x_j}$. The simplest option for this function is the dot product:\n",
        "$$\\mathbf{w}_{ij}{'} = \\mathbf{x}_i^T\\mathbf{x}_j$$\n",
        "Note that $\\mathbf{x_i}$ is the input vector at the same position as the current output vector $\\mathbf{y_i}$. For the next output vector, we get an entirely new series of dot products, and a different weighted sum.<br><br>\n",
        "The dot product gives us a value anywhere between negative and positive infinity, so we apply a softmax to map the values to $[0, 1]$ and to ensure tht they sum to $1$ over the whole sequence:\n",
        "$$\\mathbf{w}_{ij} = \\frac{\\exp  \\mathbf{w}_{ij}{'} }{\\sum_j \\exp \\mathbf{w}_{ij}'}$$\n",
        "\n",
        "Assigning embeddings vectors $v_t$ to each word $t$ in our vocabulory is known as *embedding layer*, the values of the embedding vectors will be learned. It turns the word sequence *the, cat, walks, on, the, street* into vector space\n",
        "$$\\mathbf{v}_{the}, \\mathbf{v}_{cat}, \\mathbf{v}_{walks}, \\mathbf{v}_{on}, \\mathbf{v}_{the}, \\mathbf{v}_{street}$$\n",
        "When this sequence is fed into the self-attention layer, the output is another sequence of vectors<br>\n",
        "$$\\mathbf{y}_{the}, \\mathbf{y}_{cat}, \\mathbf{y}_{walks}, \\mathbf{y}_{on}, \\mathbf{y}_{the}, \\mathbf{y}_{street}$$\n",
        "where $\\mathbf{y}_{cat}$ is a weighted sum over all the embedding vectors in the first sequence, weighted by their(normalize) dot-product with $\\mathbf{v}_{cat}$.<br><br><br>\n",
        "\n",
        "Since we are *learning* what the values in $\\mathbf{v}_t$ should be, how \"related\" two words are is entirely determined by the task. In most case, the definite articles *the* is not very relevant to the interpretation of the other words in the sentence; therefor we will likely end up with an embedding $\\mathbf{v}_{the}$ that has a low or negative dot product with all other words. On the other hand, to interpret what *walks* means in this sentence, it's very helpful to work out *who* is doing the walking. This is likely expressed by a noun, so for nounds like *cat* and verbs like *walks*, we will likely learn embeddings $\\mathbf{v}_{cat}$ and $\\mathbf{v}_{walks}$ that have a high, positive dot product together.<br><br><br>\n",
        "This is the basic intuition behind self-attention. The dot product expresses how related two vectors in the input seqeunce are, with \"related\" defined by the learning task, and the output vectors are weighted sums over the whhoel input sequence, with the weights determined by these dot products.<br><br><br>\n",
        "Before we move on, it's worthwhile to note the following properties, which are unsual for a sequence-to-sequence operation:\n",
        "*   There are no parameters(yet). What the basic self-attention actually does is entirely determined by whatever mechanism creates the input sequence. Upstream mechanisms, like an embedding layer, drive the self-attention by learning representaions with particular dot products (although we'll add a few parameters later).\n",
        "*    Self attention sees its input as a set, not a sequence. If we permute the input sequence, the output sequence will be exactly the same, except permuted also (i.e. self-attention is permutation equivariant). We will mitigate this somewhat when we build the full transformer, but the self-attention by itself actually ignores the sequential nature of the input\n"
      ],
      "metadata": {
        "id": "HgyjMaCblZae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The whole self-attention equation can be written as:\n",
        "$$\\begin{align*}\n",
        "\\mathbf{y}_i = \\displaystyle \\sum_j \\left\\{ \\frac{\\exp \\mathbf{x}_i^T \\mathbf{x}_k}{\\sum_k \\exp \\mathbf{x}_i^T \\mathbf{x}_k} \\right\\} \\mathbf{x}_j\n",
        "\\end{align*}$$<br><br>\n",
        "$k, j$ are both same, indexes over the same sequence. <br><br>This implies, when the sentences is given to the self-attention, in the form of sequence of input vectors and corresponding output vectors are generated. The corresponding output vector is sum of probability of each input vector multiplied(hadamard/dot product) with actual input vector. The probabilities are generated by dot product of corresponding input vector(transposed) with all other input vectors tokens. The way input vectors are arranged is learned by the self-attention."
      ],
      "metadata": {
        "id": "eNIu_ZbfxJeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **In Pytorch: Basic Self-Attention**\n"
      ],
      "metadata": {
        "id": "q9vJBE8wxJZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let the input be a sequence of $\\mathbf{t}$ vectors with $\\mathbf{k}$ dimensions as $\\mathbf{t}$ by $\\mathbf{k}$ matrix $\\mathbf{X}$. Including a minibatch dimension b, gives us an input tensor of size ($\\mathbf{b, t, k}$). <br><br>\n",
        "\n",
        "The set of all dor products for $\\mathbf{w}_{ij}^{'} $ forms a matrix, which we can be computed simply by multiplying $\\mathbf{X}$ by its transpose\n",
        "\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# assume we have some tensor x with size (b, t, k)\n",
        "x = ...\n",
        "\n",
        "raw_weights = torch.bmm(x, x.transpose(1,2))\n",
        "# - torch.bmm is a batched matrix mulitplication. It\n",
        "# applies matrix multiplication over batches of Matrices\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "CDq0vBM3aprB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, to turn the raw weights $\\mathbf{w}_{ij}^{'}$ into positive values tht sum to one, we apply a row-wise softmax:\n",
        "\n",
        "```python\n",
        "weights = F.softmax(raw_weights, dim=2)\n",
        "```\n",
        "Finally, to compute the output sequence, we just multiply the weight matrix by $\\mathbf{X}$. this results in a batch of output matrices $\\mathbf{Y}$ of size ($\\mathbf{b, t, k}$) whose rows are weighted sums over the rows of $\\mathbf{X}$.\n",
        "\n",
        "\n",
        "```python\n",
        "y = torch.bmm(weights, x)\n",
        "```\n",
        "basic self-attention consists of two matrix multiplications & one softmax."
      ],
      "metadata": {
        "id": "8ZBvUVXNaKNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Additional Tricks**"
      ],
      "metadata": {
        "id": "lwgdkNwSdG_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Query, Keys and Values**"
      ],
      "metadata": {
        "id": "78CzkouMRhFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The input $\\mathbf{x}_i$ is used in threee(3) different ways:<br>\n",
        "1.  It is compared with every other vector to create the weights for its own output $\\mathbf{y}_i$, it is called as **query** $\\implies$ $\\mathbf{q}_i = \\mathbf{W}_q \\mathbf{x}_i$\n",
        "2.  It is compared with every other vector to create the weights for the outputof the $j$-th vector $\\mathbf{y}_j$, it is called as **key** $\\implies$$\\mathbf{k}_i = \\mathbf{W}_k \\mathbf{x}_i$\n",
        "3.  It is used as part of the weighted sum to compute each output vector once the weights have been created, it is called as **value** $\\implies$ $\\mathbf{v}_i = \\mathbf{W}_v \\mathbf{x}_i$\n",
        "\n",
        "$$\\mathbf{q}_i = \\mathbf{W}_q \\mathbf{x}_i \\qquad \\mathbf{k}_i = \\mathbf{W}_k \\mathbf{x}_i \\qquad \\mathbf{v}_i = \\mathbf{W}_v \\mathbf{x}_i\\\\\n",
        "\\mathbf{w}_{ij}^{'} = \\mathbf{q}_i^T\\mathbf{k}_j\\\\\n",
        "\\mathbf{w}_{ij} = \\text{softmax}(\\mathbf{w}_{ij}^{'})\\\\\n",
        "\\mathbf{y}_i = \\sum_j \\mathbf{w}_{ij} \\mathbf{v}_j$$<br><br>"
      ],
      "metadata": {
        "id": "5D_3-DjxfPUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Scaling the dot product**"
      ],
      "metadata": {
        "id": "tcBtILpERs5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To stop the softmax gradients from growing too large and slow down the learning. The average value of the dot product grows with embedding dimentions k, it helps to scale down the growing softmax function\n",
        "$$\\mathbf{w}_{ij}^{'} = \\dfrac{\\mathbf{q}_i^T \\mathbf{k}_j}{\\sqrt{k}}$$\n",
        "why k? its Euclidean length take from $\\mathbb{R}^k$"
      ],
      "metadata": {
        "id": "XYBf3hEufPPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Multi-head attention**"
      ],
      "metadata": {
        "id": "zi_HP6M-RxNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A word can mean different things to different neighbours. \"*The quick brown fox jumps over the lazy dog*\". We see word \"*jumps*\" had different relations to different parts of the sentence. \"*fox*\" express who doing the action and \"*dog*\" represents an animate object that was beneath action was performed. <br><br>\n",
        "The all the information is summed together in the single-attention. The inputs $\\mathbf{x}_{fox}$ and $\\mathbf{x}_{dog}$ can influence the output $\\mathbf{y}_{jump}$ by different amounts, depending on their dot-product with $\\mathbf{x}_{jump} $ but they can't influence it in different ways. If we want the similar different information, we can achieve this by giving combining several self-attention mechanisms (which we'll index with $r$), each with different matrices $\\mathbf{W}_q^r, \\mathbf{W}_k^r, \\mathbf{W}_v^r$. These are called as *attention heads.*<br><br>\n",
        "For input $\\mathbf{x}_i$ each attention head produces a different output vector $\\mathbf{y}_i^r$. We concatenate these, and pass them through a linear transformation to reduce the dimension back to $k$.<br><br>\n",
        "The simplest way to understand multi-head self-attention is to see it as a small number of copies of the self-attention mechanisam applied in parallel, each with their own key, value and query transformation. This works well, but for $\\mathbf{R}$ heads, the self-attention operation is $\\mathbf{R}$ times as slow. There is a way to implement multi-head self-attention so that it is roughly as fast as the single-head version, but we still get the benefit of having different self-attention operations in parallel. To accomlish this, each head receives low-dimensinal keys, queries and values. If the input vecor has $k = 256$ dimensions, and we have $h = 4$ attention heads, we multiply the input vectors by a $256 \\times 64$ matrix to project them down to a sequence of $64$ dimensional vectors. For every head, we do this 3 times: for the keys, the queries and the values.<br><br>\n",
        "\n",
        "This requires $3h$ matrices of size $k$ by $k/h$. In total, this gives us $3hk \\dfrac{k}{h} = 3k^2$ parameters to compute the inputs to the multi-head self-attention; the same as we had for the single-head self-attention. We can even implement this with just three $ k \\times k$ matrix multiplications as in the single-head self-attention. the only extra operation we need is to slice the resulting sequence of vectors into chunks"
      ],
      "metadata": {
        "id": "hURmt_YW0yad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **In Pytorch: complete self-attention**"
      ],
      "metadata": {
        "id": "Zh2lkKksGkis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement a self-attention module using Pytorch module<br>\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.fuctional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, k, heads=4, mask=False):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        assert k % heads == 0\n",
        "\n",
        "        self.k, self.heads = k, heads\n",
        "```\n",
        "Assert imposes that embeddings dimensions should be divisible by the number of heads. We will setup some linear transformations with emb by emb matrices. The \"nn.Linear\" module with diables bias gives us such a projection\n",
        "\n",
        "\n",
        "```python\n",
        "# These compute the queries, keys and vlaues for all\n",
        "# heads\n",
        "\n",
        "    self.tokeys = nn.Linear(k, k, bias=False)\n",
        "    self.toqueries = nn.Linear(k, k, bias=False)\n",
        "    self.tovlaues = nn.Linear(k, k, bias=False)\n",
        "\n",
        "    # This will be applied after the multi-head self-attention operation\n",
        "    self.unifyheads = nn.Linear(k, k)\n",
        "\n",
        "```\n",
        "Lets compute queries, keys, and values for all heads as the part of forward function and also implements the self-attention\n",
        "\n",
        "\n",
        "```python\n",
        "    def forward(self, x):\n",
        "        b, t, k = x.size()\n",
        "        h = self.heads\n",
        "\n",
        "        queries = self.toqueries(x)\n",
        "        keys = self.tokeys(x)\n",
        "        values = self.tovalues(x)\n",
        "```\n",
        "This gives us three vectors sequence of the full embedding dimensions $k$. As we saw above we can now cut these into $h$ chunks.\n",
        "\n",
        "\n",
        "```python\n",
        "        s = k // h\n",
        "\n",
        "        keys = keys.view(b, t, h, s)\n",
        "        queries = queries.view(b, t, h, s)\n",
        "        values = values.view(b, t, h, s)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l-K-fzA2GtFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This simply reshapes the tensors to add a dimension that iterate over the heads. For a singel vector in our sequence we can think of it as reshaping a vector of dimensions $k$ into a matrix of $h$ by $k // h$.\n",
        "We need to compute the dor products. This is the same operation for every head, so we fold the heads into the batch dimensions. This ensures that we can use torch.bmm() as before, and the whole collection of keys, queries and values will just be seen as a slightly larger batch.<br><br>\n",
        "Since the head and batch dimension are not next to each other, we need to transpose before we reshape.(This is costly, ut it seems to be unavoidable.)\n",
        "\n",
        "\n",
        "```python\n",
        "        # - fold heads into the batch dimension\n",
        "        keys = keys.transpose(1, 2).contigous().view(b * h, t, s)\n",
        "        queries = queries.transpose(1, 2).contigous().view(b * h, t, s)\n",
        "        values = values.transpose(1, 2).contigous().view(b * h, t, s)\n",
        "```\n",
        "As before, the dot products can be computed in a single matrix multiplication, but now between the queries and the keys.\n",
        "\n",
        "\n",
        "```python\n",
        "        # Get dot products of queries and keys, and scale\n",
        "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
        "        # -- dot has size (b*h, t, s) containing raw weights\n",
        "\n",
        "        # scale the dot product\n",
        "        dot = dot / (k ** (1/2))\n",
        "\n",
        "        # normalize\n",
        "        dot = F.softmax(dot, dim=2)\n",
        "        # - dot now contains row-wise normalized weights\n",
        "```\n",
        "We apply the self attention weights dot to the values, giving us the output for each attention head\n",
        "\n",
        "\n",
        "```python\n",
        "        # apply the self attention to the values\n",
        "        out = torch.bmm(dot, values).view(b, h, t, s)\n",
        "```\n",
        "\n",
        "To unify the attention heads, we transpose again, so that the head dimension and the embedding dimension are next to each other, and reshape to get concatenated vectors of dimension $e$. We then pass these through the unifyheads layer for a final projection\n",
        "\n",
        "\n",
        "```python\n",
        "        # swap h, t back, unify heads\n",
        "        out = out.transpose(1, 2).contigous().view(b, t, s * h)\n",
        "\n",
        "        return self.unifyheads(out)\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oPVzXJtdQxQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bCfK2FPHbXyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References:**\n",
        "1.   https://peterbloem.nl/blog/transformers\n",
        "2.   "
      ],
      "metadata": {
        "id": "aCib_8-nI4OW"
      }
    }
  ]
}
